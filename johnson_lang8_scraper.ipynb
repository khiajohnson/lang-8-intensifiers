{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lang-8 Web Scraper\n",
    "\n",
    "LING 530B <br>\n",
    "Khia Johnson <br>\n",
    "April 15, 2019\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_as_soup(url):  \n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < 3:\n",
    "        try:\n",
    "            soup = BeautifulSoup(urlopen(url).read().decode('utf-8', errors='ignore'), 'html.parser')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('fail to load #', attempts)\n",
    "            repr(e)\n",
    "            attempts+=1\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_profile_info(page_soup):\n",
    "    rows = page_soup.find_all('div', {'class':'user_profile_row'})\n",
    "    user_details = {}  \n",
    "    \n",
    "    for r in rows:\n",
    "        profile_row = r.text.strip('\\n').split('\\n')\n",
    "        user_details[profile_row[0].replace(' ','_')] = profile_row[1]   \n",
    "    \n",
    "    user_details['L1'] = page_soup.find('dd', {'class':'speaking_lang_name'}).text\n",
    "    user_details['L2'] = page_soup.find('dd', {'class':'studying_lang_name'}).text.split()\n",
    "    try:\n",
    "        user_details['Entries_written'] = page_soup.find('td', {'class':'l01'}).a.text\n",
    "    except:\n",
    "        user_details['Entries_written'] = \"0\"\n",
    "    try:\n",
    "        user_details['Corrections_made'] = page_soup.find('td', {'class':'l02'}).a.text\n",
    "    except:\n",
    "        user_details['Corrections_made'] = \"0\"\n",
    "    try:\n",
    "        user_details['Corrections_rcvd'] = page_soup.find('td', {'class':'l03'}).a.text \n",
    "    except:\n",
    "        user_details['Corrections_rcvd'] = \"0\"\n",
    "        \n",
    "    return user_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagination(page_soup):\n",
    "    try:\n",
    "        pages = []\n",
    "        for p in page_soup.find('ul', {'class':'pagination'}):\n",
    "            try:\n",
    "                pages.append(int(p.string))\n",
    "            except:\n",
    "                pass\n",
    "        return max(pages) \n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_page_friends_info(page_soup):   \n",
    "    friend_soup = page_soup.find('ul', {'class':'list_fliend'}).find_all('div',{'class':'column cfx'})   \n",
    "    keep = {}    \n",
    "\n",
    "    for f in friend_soup:\n",
    "        details = {}\n",
    "        friend_id = f.find('a').get('href').replace('/','')\n",
    "        details['L1'] = f.find('li', {'class':'speaking'}).string\n",
    "        if 'English' in f.find('li', {'class':'studying study_column'}).string.replace('\\n','').split(', '):\n",
    "            details['L2_English'] = True\n",
    "        else: \n",
    "            details['L2_English'] = False\n",
    "        details['friend_num'] = f.find('li', {'class':'friend_num'}).string\n",
    "        try: \n",
    "            details['loc'] = f.find('div', {'class':'location'}).string              \n",
    "        except: \n",
    "            details['loc'] = None  \n",
    "        keep[friend_id] = details \n",
    "        \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_page_journal_urls(page_soup):\n",
    "    journal_soup = page_soup.find_all('div',{'class':'vertical-spaced journals_flex floated_on_right'})\n",
    "    urls = []\n",
    "    \n",
    "    for entry in journal_soup: \n",
    "        try:\n",
    "            if entry.find('li', {'class':'studying'}).text.strip() == 'English':\n",
    "                if entry.find('a', {'class':'premium'}) is not None:\n",
    "                    urls.append(entry.find_all('a')[2].get('href'))\n",
    "                else:\n",
    "                    urls.append(entry.find_all('a')[1].get('href'))\n",
    "            else: \n",
    "                pass\n",
    "        except:\n",
    "            print(user, 'entry language not identified')\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_entry_and_comments(page_soup, user_id): \n",
    "    entry = {}\n",
    "    entry['entry_text'] = page_soup.find('div', {'id':'body_show_ori'}).get_text(\" \").strip()\n",
    "    entry['entry_date'] = page_soup.find_all('span', {'class':'journal_date floated_on_left'})[0].text.strip()\n",
    "\n",
    "    comments = []\n",
    "    comment_soup = page_soup.find_all('div',{'class':'journal_comment_header'})\n",
    "    for c in comment_soup:\n",
    "        if user_id == c.find('a').get('href').replace('/',''):\n",
    "            comment_date = c.text.strip().split('\\n')[0]\n",
    "            comment_text = c.next_sibling.next_sibling.text.strip()\n",
    "            comments.append((comment_date, comment_text))\n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    entry['comments'] = comments\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write initial state for states files\n",
    "# Commented out-- don't re-run\n",
    "#todo_user_ids = ['1784120','1122476','1677477','225463','1015012','1805882','1636158']\n",
    "#users_scraped = []\n",
    "#users_failed = []\n",
    "\n",
    "#with open('530_project_lists.txt', 'w') as file:\n",
    "#        json.dump({'to-do': todo_user_ids, 'scraped': users_scraped, 'failed': users_failed}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main scraping loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load current state and reset error counter\n",
    "with open('530_project_lists.txt', 'r') as file:\n",
    "    state = json.loads(file.read())\n",
    "\n",
    "todo_user_ids = state['to-do']\n",
    "users_scraped = state['scraped']\n",
    "users_failed = state['failed']\n",
    "errors = 0\n",
    "\n",
    "# Main scraping loop\n",
    "while True:\n",
    "    start = time.time()\n",
    "    user = todo_user_ids.pop()\n",
    "    if user in todo_user_ids:\n",
    "        continue\n",
    "    \n",
    "    base_url = 'https://lang-8.com/' + user\n",
    "    try:\n",
    "        friend_soup = open_as_soup(base_url + '/friends')\n",
    "        profile = extract_profile_info(friend_soup)\n",
    "        \n",
    "        friend_pages = get_pagination(friend_soup)\n",
    "        friends = collect_page_friends_info(friend_soup)\n",
    "        if friend_pages > 1: \n",
    "            for p in range(2, friend_pages + 1): \n",
    "                p_soup = open_as_soup(base_url + '/friends' + '?page=' + str(p))\n",
    "                friends.update(collect_page_friends_info(p_soup))   \n",
    "            \n",
    "        if 'Age' not in profile.keys():\n",
    "            users_failed.append(user)\n",
    "            for f in friends:\n",
    "                if friends[f]['L2_English'] == True and friends[f]['loc'] is not None and f not in users_scraped and f not in users_failed:\n",
    "                    todo_user_ids.append(f)\n",
    "            print(user,'missing age, seconds:',time.time()-start)\n",
    "            continue\n",
    "    \n",
    "        journal_soup = open_as_soup(base_url + '/journals')\n",
    "        journal_urls = collect_page_journal_urls(journal_soup)\n",
    "        journal_pages = get_pagination(friend_soup) \n",
    "        \n",
    "        if journal_pages > 1:\n",
    "            for p in range(2, journal_pages + 1): \n",
    "                p_soup = open_as_soup(base_url + '/journals' + '?page=' + str(p))\n",
    "                journal_urls.extend(collect_page_journal_urls(p_soup))\n",
    "        \n",
    "        journal = {}\n",
    "        for entry_url in journal_urls:\n",
    "            entry_soup = open_as_soup(entry_url)\n",
    "            journal[entry_url] = get_one_entry_and_comments(entry_soup, user)\n",
    "    \n",
    "        if journal == {}:\n",
    "            print(user, 'has no English entries, seconds:',time.time()-start)\n",
    "            users_failed.append(user)\n",
    "            for f in friends:\n",
    "                if friends[f]['L2_English'] == True and friends[f]['loc'] is not None and f not in users_scraped and f not in users_failed:\n",
    "                    todo_user_ids.append(f)\n",
    "            continue\n",
    "    \n",
    "        data = {user: {'profile': profile, \n",
    "                       'friends': friends, \n",
    "                       'journal': journal}}\n",
    "        \n",
    "        with open('530_project_data.txt', 'a') as file:\n",
    "            json.dump(data, file)\n",
    "            file.write('\\n')\n",
    "        \n",
    "        users_scraped.append(user)\n",
    "        for f in friends:\n",
    "            if friends[f]['L2_English'] == True and friends[f]['loc'] is not None and f not in users_scraped and f not in users_failed:\n",
    "                todo_user_ids.append(f)\n",
    "            \n",
    "        with open('530_project_lists.txt', 'w') as file:\n",
    "            json.dump({'to-do': todo_user_ids, \n",
    "                       'scraped': users_scraped, \n",
    "                       'failed': users_failed}, file)\n",
    "    \n",
    "        print(user + \" seconds:\", round(time.time()-start, 2), ', users scraped:',len(set(users_scraped)))\n",
    "\n",
    "    except:\n",
    "        print('error with: ' + user + ', users scraped:', len(set(users_scraped)))\n",
    "        users_failed.append(user)\n",
    "        \n",
    "        with open('530_project_lists.txt', 'w') as file:\n",
    "            json.dump({'to-do': todo_user_ids, \n",
    "                       'scraped': users_scraped, \n",
    "                       'failed': users_failed}, file)\n",
    "        errors+=1\n",
    "        if errors>1:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
